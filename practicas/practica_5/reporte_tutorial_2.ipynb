{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reporte_tutorial_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Jnup01VBlV"
      },
      "source": [
        "### Reporte Practica 5 - Tutorial 2\n",
        "### Autor: Javier Abarca Jimenez \n",
        "### Carn√©: B70018\n",
        "### Tutorial: [Neural Network Models for Combined Classification and Regression](https://machinelearningmastery.com/neural-network-models-for-combined-classification-and-regression/) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuKAhWwbkC-o"
      },
      "source": [
        "## Abalone Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3tzwrE3hPEz",
        "outputId": "3474bd5f-cb08-4b6c-84a3-927e253a02e2"
      },
      "source": [
        "# load and summarize the abalone dataset\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "# summarize shape\n",
        "print(dataframe.shape)\n",
        "# summarize first few lines\n",
        "print(dataframe.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4177, 9)\n",
            "   0      1      2      3       4       5       6      7   8\n",
            "0  M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150  15\n",
            "1  M  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070   7\n",
            "2  F  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210   9\n",
            "3  M  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155  10\n",
            "4  I  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055   7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMx23FkykXJg"
      },
      "source": [
        "## Regression Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iifsVgyUlfVQ",
        "outputId": "5371ce57-0c3c-4ca8-a7f2-258da6662826"
      },
      "source": [
        "# regression mlp model for the abalone dataset\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "# compile the keras model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "error = mean_absolute_error(y_test, yhat)\n",
        "print('MAE: %.3f' % error)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "88/88 - 1s - loss: 83.3248\n",
            "Epoch 2/150\n",
            "88/88 - 0s - loss: 31.4414\n",
            "Epoch 3/150\n",
            "88/88 - 0s - loss: 10.6724\n",
            "Epoch 4/150\n",
            "88/88 - 0s - loss: 9.6970\n",
            "Epoch 5/150\n",
            "88/88 - 0s - loss: 9.2217\n",
            "Epoch 6/150\n",
            "88/88 - 0s - loss: 8.8023\n",
            "Epoch 7/150\n",
            "88/88 - 0s - loss: 8.4100\n",
            "Epoch 8/150\n",
            "88/88 - 0s - loss: 8.0654\n",
            "Epoch 9/150\n",
            "88/88 - 0s - loss: 7.7422\n",
            "Epoch 10/150\n",
            "88/88 - 0s - loss: 7.4658\n",
            "Epoch 11/150\n",
            "88/88 - 0s - loss: 7.2402\n",
            "Epoch 12/150\n",
            "88/88 - 0s - loss: 7.0564\n",
            "Epoch 13/150\n",
            "88/88 - 0s - loss: 6.9057\n",
            "Epoch 14/150\n",
            "88/88 - 0s - loss: 6.7851\n",
            "Epoch 15/150\n",
            "88/88 - 0s - loss: 6.6821\n",
            "Epoch 16/150\n",
            "88/88 - 0s - loss: 6.5915\n",
            "Epoch 17/150\n",
            "88/88 - 0s - loss: 6.5077\n",
            "Epoch 18/150\n",
            "88/88 - 0s - loss: 6.4274\n",
            "Epoch 19/150\n",
            "88/88 - 0s - loss: 6.3497\n",
            "Epoch 20/150\n",
            "88/88 - 0s - loss: 6.2666\n",
            "Epoch 21/150\n",
            "88/88 - 0s - loss: 6.2071\n",
            "Epoch 22/150\n",
            "88/88 - 0s - loss: 6.1234\n",
            "Epoch 23/150\n",
            "88/88 - 0s - loss: 6.0565\n",
            "Epoch 24/150\n",
            "88/88 - 0s - loss: 5.9889\n",
            "Epoch 25/150\n",
            "88/88 - 0s - loss: 5.9325\n",
            "Epoch 26/150\n",
            "88/88 - 0s - loss: 5.8599\n",
            "Epoch 27/150\n",
            "88/88 - 0s - loss: 5.7864\n",
            "Epoch 28/150\n",
            "88/88 - 0s - loss: 5.7397\n",
            "Epoch 29/150\n",
            "88/88 - 0s - loss: 5.6599\n",
            "Epoch 30/150\n",
            "88/88 - 0s - loss: 5.6005\n",
            "Epoch 31/150\n",
            "88/88 - 0s - loss: 5.5369\n",
            "Epoch 32/150\n",
            "88/88 - 0s - loss: 5.4930\n",
            "Epoch 33/150\n",
            "88/88 - 0s - loss: 5.4350\n",
            "Epoch 34/150\n",
            "88/88 - 0s - loss: 5.4073\n",
            "Epoch 35/150\n",
            "88/88 - 0s - loss: 5.3338\n",
            "Epoch 36/150\n",
            "88/88 - 0s - loss: 5.2901\n",
            "Epoch 37/150\n",
            "88/88 - 0s - loss: 5.2253\n",
            "Epoch 38/150\n",
            "88/88 - 0s - loss: 5.2029\n",
            "Epoch 39/150\n",
            "88/88 - 0s - loss: 5.1735\n",
            "Epoch 40/150\n",
            "88/88 - 0s - loss: 5.1161\n",
            "Epoch 41/150\n",
            "88/88 - 0s - loss: 5.1093\n",
            "Epoch 42/150\n",
            "88/88 - 0s - loss: 5.0446\n",
            "Epoch 43/150\n",
            "88/88 - 0s - loss: 5.0081\n",
            "Epoch 44/150\n",
            "88/88 - 0s - loss: 4.9868\n",
            "Epoch 45/150\n",
            "88/88 - 0s - loss: 4.9586\n",
            "Epoch 46/150\n",
            "88/88 - 0s - loss: 4.9362\n",
            "Epoch 47/150\n",
            "88/88 - 0s - loss: 4.9251\n",
            "Epoch 48/150\n",
            "88/88 - 0s - loss: 4.8780\n",
            "Epoch 49/150\n",
            "88/88 - 0s - loss: 4.8888\n",
            "Epoch 50/150\n",
            "88/88 - 0s - loss: 4.8499\n",
            "Epoch 51/150\n",
            "88/88 - 0s - loss: 4.8337\n",
            "Epoch 52/150\n",
            "88/88 - 0s - loss: 4.8370\n",
            "Epoch 53/150\n",
            "88/88 - 0s - loss: 4.8075\n",
            "Epoch 54/150\n",
            "88/88 - 0s - loss: 4.7980\n",
            "Epoch 55/150\n",
            "88/88 - 0s - loss: 4.8154\n",
            "Epoch 56/150\n",
            "88/88 - 0s - loss: 4.8113\n",
            "Epoch 57/150\n",
            "88/88 - 0s - loss: 4.7995\n",
            "Epoch 58/150\n",
            "88/88 - 0s - loss: 4.7598\n",
            "Epoch 59/150\n",
            "88/88 - 0s - loss: 4.7791\n",
            "Epoch 60/150\n",
            "88/88 - 0s - loss: 4.7449\n",
            "Epoch 61/150\n",
            "88/88 - 0s - loss: 4.7675\n",
            "Epoch 62/150\n",
            "88/88 - 0s - loss: 4.7422\n",
            "Epoch 63/150\n",
            "88/88 - 0s - loss: 4.7277\n",
            "Epoch 64/150\n",
            "88/88 - 0s - loss: 4.7296\n",
            "Epoch 65/150\n",
            "88/88 - 0s - loss: 4.7299\n",
            "Epoch 66/150\n",
            "88/88 - 0s - loss: 4.7790\n",
            "Epoch 67/150\n",
            "88/88 - 0s - loss: 4.7068\n",
            "Epoch 68/150\n",
            "88/88 - 0s - loss: 4.6942\n",
            "Epoch 69/150\n",
            "88/88 - 0s - loss: 4.7008\n",
            "Epoch 70/150\n",
            "88/88 - 0s - loss: 4.6872\n",
            "Epoch 71/150\n",
            "88/88 - 0s - loss: 4.7034\n",
            "Epoch 72/150\n",
            "88/88 - 0s - loss: 4.6745\n",
            "Epoch 73/150\n",
            "88/88 - 0s - loss: 4.7039\n",
            "Epoch 74/150\n",
            "88/88 - 0s - loss: 4.6945\n",
            "Epoch 75/150\n",
            "88/88 - 0s - loss: 4.6801\n",
            "Epoch 76/150\n",
            "88/88 - 0s - loss: 4.6903\n",
            "Epoch 77/150\n",
            "88/88 - 0s - loss: 4.6621\n",
            "Epoch 78/150\n",
            "88/88 - 0s - loss: 4.6641\n",
            "Epoch 79/150\n",
            "88/88 - 0s - loss: 4.6646\n",
            "Epoch 80/150\n",
            "88/88 - 0s - loss: 4.6556\n",
            "Epoch 81/150\n",
            "88/88 - 0s - loss: 4.6693\n",
            "Epoch 82/150\n",
            "88/88 - 0s - loss: 4.6795\n",
            "Epoch 83/150\n",
            "88/88 - 0s - loss: 4.6443\n",
            "Epoch 84/150\n",
            "88/88 - 0s - loss: 4.6343\n",
            "Epoch 85/150\n",
            "88/88 - 0s - loss: 4.6618\n",
            "Epoch 86/150\n",
            "88/88 - 0s - loss: 4.6391\n",
            "Epoch 87/150\n",
            "88/88 - 0s - loss: 4.6449\n",
            "Epoch 88/150\n",
            "88/88 - 0s - loss: 4.6390\n",
            "Epoch 89/150\n",
            "88/88 - 0s - loss: 4.6413\n",
            "Epoch 90/150\n",
            "88/88 - 0s - loss: 4.6467\n",
            "Epoch 91/150\n",
            "88/88 - 0s - loss: 4.6341\n",
            "Epoch 92/150\n",
            "88/88 - 0s - loss: 4.6338\n",
            "Epoch 93/150\n",
            "88/88 - 0s - loss: 4.6577\n",
            "Epoch 94/150\n",
            "88/88 - 0s - loss: 4.6256\n",
            "Epoch 95/150\n",
            "88/88 - 0s - loss: 4.6102\n",
            "Epoch 96/150\n",
            "88/88 - 0s - loss: 4.6275\n",
            "Epoch 97/150\n",
            "88/88 - 0s - loss: 4.6344\n",
            "Epoch 98/150\n",
            "88/88 - 0s - loss: 4.6141\n",
            "Epoch 99/150\n",
            "88/88 - 0s - loss: 4.6228\n",
            "Epoch 100/150\n",
            "88/88 - 0s - loss: 4.6375\n",
            "Epoch 101/150\n",
            "88/88 - 0s - loss: 4.6158\n",
            "Epoch 102/150\n",
            "88/88 - 0s - loss: 4.6137\n",
            "Epoch 103/150\n",
            "88/88 - 0s - loss: 4.6256\n",
            "Epoch 104/150\n",
            "88/88 - 0s - loss: 4.6098\n",
            "Epoch 105/150\n",
            "88/88 - 0s - loss: 4.6503\n",
            "Epoch 106/150\n",
            "88/88 - 0s - loss: 4.6134\n",
            "Epoch 107/150\n",
            "88/88 - 0s - loss: 4.6088\n",
            "Epoch 108/150\n",
            "88/88 - 0s - loss: 4.6109\n",
            "Epoch 109/150\n",
            "88/88 - 0s - loss: 4.5998\n",
            "Epoch 110/150\n",
            "88/88 - 0s - loss: 4.6077\n",
            "Epoch 111/150\n",
            "88/88 - 0s - loss: 4.6170\n",
            "Epoch 112/150\n",
            "88/88 - 0s - loss: 4.6266\n",
            "Epoch 113/150\n",
            "88/88 - 0s - loss: 4.6061\n",
            "Epoch 114/150\n",
            "88/88 - 0s - loss: 4.5837\n",
            "Epoch 115/150\n",
            "88/88 - 0s - loss: 4.5997\n",
            "Epoch 116/150\n",
            "88/88 - 0s - loss: 4.5960\n",
            "Epoch 117/150\n",
            "88/88 - 0s - loss: 4.5825\n",
            "Epoch 118/150\n",
            "88/88 - 0s - loss: 4.6002\n",
            "Epoch 119/150\n",
            "88/88 - 0s - loss: 4.5909\n",
            "Epoch 120/150\n",
            "88/88 - 0s - loss: 4.5919\n",
            "Epoch 121/150\n",
            "88/88 - 0s - loss: 4.5926\n",
            "Epoch 122/150\n",
            "88/88 - 0s - loss: 4.6055\n",
            "Epoch 123/150\n",
            "88/88 - 0s - loss: 4.5766\n",
            "Epoch 124/150\n",
            "88/88 - 0s - loss: 4.5843\n",
            "Epoch 125/150\n",
            "88/88 - 0s - loss: 4.5901\n",
            "Epoch 126/150\n",
            "88/88 - 0s - loss: 4.6081\n",
            "Epoch 127/150\n",
            "88/88 - 0s - loss: 4.5712\n",
            "Epoch 128/150\n",
            "88/88 - 0s - loss: 4.5864\n",
            "Epoch 129/150\n",
            "88/88 - 0s - loss: 4.5794\n",
            "Epoch 130/150\n",
            "88/88 - 0s - loss: 4.5934\n",
            "Epoch 131/150\n",
            "88/88 - 0s - loss: 4.5832\n",
            "Epoch 132/150\n",
            "88/88 - 0s - loss: 4.5744\n",
            "Epoch 133/150\n",
            "88/88 - 0s - loss: 4.5729\n",
            "Epoch 134/150\n",
            "88/88 - 0s - loss: 4.6004\n",
            "Epoch 135/150\n",
            "88/88 - 0s - loss: 4.5711\n",
            "Epoch 136/150\n",
            "88/88 - 0s - loss: 4.5696\n",
            "Epoch 137/150\n",
            "88/88 - 0s - loss: 4.5854\n",
            "Epoch 138/150\n",
            "88/88 - 0s - loss: 4.5812\n",
            "Epoch 139/150\n",
            "88/88 - 0s - loss: 4.5995\n",
            "Epoch 140/150\n",
            "88/88 - 0s - loss: 4.6025\n",
            "Epoch 141/150\n",
            "88/88 - 0s - loss: 4.5802\n",
            "Epoch 142/150\n",
            "88/88 - 0s - loss: 4.5659\n",
            "Epoch 143/150\n",
            "88/88 - 0s - loss: 4.5622\n",
            "Epoch 144/150\n",
            "88/88 - 0s - loss: 4.5968\n",
            "Epoch 145/150\n",
            "88/88 - 0s - loss: 4.5754\n",
            "Epoch 146/150\n",
            "88/88 - 0s - loss: 4.5928\n",
            "Epoch 147/150\n",
            "88/88 - 0s - loss: 4.5630\n",
            "Epoch 148/150\n",
            "88/88 - 0s - loss: 4.5680\n",
            "Epoch 149/150\n",
            "88/88 - 0s - loss: 4.5719\n",
            "Epoch 150/150\n",
            "88/88 - 0s - loss: 4.5791\n",
            "MAE: 1.541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5psnlenomJjw"
      },
      "source": [
        "MAE: 1.541"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PDVtytcllj0"
      },
      "source": [
        "## Classification Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn1IcD1EmGSz",
        "outputId": "39c6d8d4-53de-4d37-d540-dd8398abc31d"
      },
      "source": [
        "# classification mlp model for the abalone dataset\n",
        "from numpy import unique\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "n_class = len(unique(y))\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(n_class, activation='softmax'))\n",
        "# compile the keras model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "yhat = argmax(yhat, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "88/88 - 0s - loss: 3.2518\n",
            "Epoch 2/150\n",
            "88/88 - 0s - loss: 2.8985\n",
            "Epoch 3/150\n",
            "88/88 - 0s - loss: 2.5778\n",
            "Epoch 4/150\n",
            "88/88 - 0s - loss: 2.4421\n",
            "Epoch 5/150\n",
            "88/88 - 0s - loss: 2.3593\n",
            "Epoch 6/150\n",
            "88/88 - 0s - loss: 2.2963\n",
            "Epoch 7/150\n",
            "88/88 - 0s - loss: 2.2516\n",
            "Epoch 8/150\n",
            "88/88 - 0s - loss: 2.2159\n",
            "Epoch 9/150\n",
            "88/88 - 0s - loss: 2.1907\n",
            "Epoch 10/150\n",
            "88/88 - 0s - loss: 2.1727\n",
            "Epoch 11/150\n",
            "88/88 - 0s - loss: 2.1556\n",
            "Epoch 12/150\n",
            "88/88 - 0s - loss: 2.1440\n",
            "Epoch 13/150\n",
            "88/88 - 0s - loss: 2.1324\n",
            "Epoch 14/150\n",
            "88/88 - 0s - loss: 2.1217\n",
            "Epoch 15/150\n",
            "88/88 - 0s - loss: 2.1135\n",
            "Epoch 16/150\n",
            "88/88 - 0s - loss: 2.1033\n",
            "Epoch 17/150\n",
            "88/88 - 0s - loss: 2.0965\n",
            "Epoch 18/150\n",
            "88/88 - 0s - loss: 2.0892\n",
            "Epoch 19/150\n",
            "88/88 - 0s - loss: 2.0838\n",
            "Epoch 20/150\n",
            "88/88 - 0s - loss: 2.0755\n",
            "Epoch 21/150\n",
            "88/88 - 0s - loss: 2.0687\n",
            "Epoch 22/150\n",
            "88/88 - 0s - loss: 2.0583\n",
            "Epoch 23/150\n",
            "88/88 - 0s - loss: 2.0507\n",
            "Epoch 24/150\n",
            "88/88 - 0s - loss: 2.0447\n",
            "Epoch 25/150\n",
            "88/88 - 0s - loss: 2.0380\n",
            "Epoch 26/150\n",
            "88/88 - 0s - loss: 2.0306\n",
            "Epoch 27/150\n",
            "88/88 - 0s - loss: 2.0251\n",
            "Epoch 28/150\n",
            "88/88 - 0s - loss: 2.0195\n",
            "Epoch 29/150\n",
            "88/88 - 0s - loss: 2.0160\n",
            "Epoch 30/150\n",
            "88/88 - 0s - loss: 2.0094\n",
            "Epoch 31/150\n",
            "88/88 - 0s - loss: 2.0063\n",
            "Epoch 32/150\n",
            "88/88 - 0s - loss: 2.0013\n",
            "Epoch 33/150\n",
            "88/88 - 0s - loss: 1.9970\n",
            "Epoch 34/150\n",
            "88/88 - 0s - loss: 1.9926\n",
            "Epoch 35/150\n",
            "88/88 - 0s - loss: 1.9908\n",
            "Epoch 36/150\n",
            "88/88 - 0s - loss: 1.9883\n",
            "Epoch 37/150\n",
            "88/88 - 0s - loss: 1.9842\n",
            "Epoch 38/150\n",
            "88/88 - 0s - loss: 1.9805\n",
            "Epoch 39/150\n",
            "88/88 - 0s - loss: 1.9775\n",
            "Epoch 40/150\n",
            "88/88 - 0s - loss: 1.9766\n",
            "Epoch 41/150\n",
            "88/88 - 0s - loss: 1.9733\n",
            "Epoch 42/150\n",
            "88/88 - 0s - loss: 1.9708\n",
            "Epoch 43/150\n",
            "88/88 - 0s - loss: 1.9694\n",
            "Epoch 44/150\n",
            "88/88 - 0s - loss: 1.9673\n",
            "Epoch 45/150\n",
            "88/88 - 0s - loss: 1.9653\n",
            "Epoch 46/150\n",
            "88/88 - 0s - loss: 1.9629\n",
            "Epoch 47/150\n",
            "88/88 - 0s - loss: 1.9620\n",
            "Epoch 48/150\n",
            "88/88 - 0s - loss: 1.9598\n",
            "Epoch 49/150\n",
            "88/88 - 0s - loss: 1.9593\n",
            "Epoch 50/150\n",
            "88/88 - 0s - loss: 1.9572\n",
            "Epoch 51/150\n",
            "88/88 - 0s - loss: 1.9563\n",
            "Epoch 52/150\n",
            "88/88 - 0s - loss: 1.9558\n",
            "Epoch 53/150\n",
            "88/88 - 0s - loss: 1.9519\n",
            "Epoch 54/150\n",
            "88/88 - 0s - loss: 1.9518\n",
            "Epoch 55/150\n",
            "88/88 - 0s - loss: 1.9527\n",
            "Epoch 56/150\n",
            "88/88 - 0s - loss: 1.9513\n",
            "Epoch 57/150\n",
            "88/88 - 0s - loss: 1.9503\n",
            "Epoch 58/150\n",
            "88/88 - 0s - loss: 1.9489\n",
            "Epoch 59/150\n",
            "88/88 - 0s - loss: 1.9490\n",
            "Epoch 60/150\n",
            "88/88 - 0s - loss: 1.9496\n",
            "Epoch 61/150\n",
            "88/88 - 0s - loss: 1.9463\n",
            "Epoch 62/150\n",
            "88/88 - 0s - loss: 1.9451\n",
            "Epoch 63/150\n",
            "88/88 - 0s - loss: 1.9446\n",
            "Epoch 64/150\n",
            "88/88 - 0s - loss: 1.9438\n",
            "Epoch 65/150\n",
            "88/88 - 0s - loss: 1.9456\n",
            "Epoch 66/150\n",
            "88/88 - 0s - loss: 1.9418\n",
            "Epoch 67/150\n",
            "88/88 - 0s - loss: 1.9418\n",
            "Epoch 68/150\n",
            "88/88 - 0s - loss: 1.9430\n",
            "Epoch 69/150\n",
            "88/88 - 0s - loss: 1.9424\n",
            "Epoch 70/150\n",
            "88/88 - 0s - loss: 1.9405\n",
            "Epoch 71/150\n",
            "88/88 - 0s - loss: 1.9398\n",
            "Epoch 72/150\n",
            "88/88 - 0s - loss: 1.9386\n",
            "Epoch 73/150\n",
            "88/88 - 0s - loss: 1.9400\n",
            "Epoch 74/150\n",
            "88/88 - 0s - loss: 1.9383\n",
            "Epoch 75/150\n",
            "88/88 - 0s - loss: 1.9417\n",
            "Epoch 76/150\n",
            "88/88 - 0s - loss: 1.9386\n",
            "Epoch 77/150\n",
            "88/88 - 0s - loss: 1.9388\n",
            "Epoch 78/150\n",
            "88/88 - 0s - loss: 1.9364\n",
            "Epoch 79/150\n",
            "88/88 - 0s - loss: 1.9376\n",
            "Epoch 80/150\n",
            "88/88 - 0s - loss: 1.9363\n",
            "Epoch 81/150\n",
            "88/88 - 0s - loss: 1.9367\n",
            "Epoch 82/150\n",
            "88/88 - 0s - loss: 1.9350\n",
            "Epoch 83/150\n",
            "88/88 - 0s - loss: 1.9345\n",
            "Epoch 84/150\n",
            "88/88 - 0s - loss: 1.9348\n",
            "Epoch 85/150\n",
            "88/88 - 0s - loss: 1.9375\n",
            "Epoch 86/150\n",
            "88/88 - 0s - loss: 1.9364\n",
            "Epoch 87/150\n",
            "88/88 - 0s - loss: 1.9337\n",
            "Epoch 88/150\n",
            "88/88 - 0s - loss: 1.9340\n",
            "Epoch 89/150\n",
            "88/88 - 0s - loss: 1.9321\n",
            "Epoch 90/150\n",
            "88/88 - 0s - loss: 1.9332\n",
            "Epoch 91/150\n",
            "88/88 - 0s - loss: 1.9341\n",
            "Epoch 92/150\n",
            "88/88 - 0s - loss: 1.9318\n",
            "Epoch 93/150\n",
            "88/88 - 0s - loss: 1.9314\n",
            "Epoch 94/150\n",
            "88/88 - 0s - loss: 1.9308\n",
            "Epoch 95/150\n",
            "88/88 - 0s - loss: 1.9297\n",
            "Epoch 96/150\n",
            "88/88 - 0s - loss: 1.9305\n",
            "Epoch 97/150\n",
            "88/88 - 0s - loss: 1.9313\n",
            "Epoch 98/150\n",
            "88/88 - 0s - loss: 1.9313\n",
            "Epoch 99/150\n",
            "88/88 - 0s - loss: 1.9294\n",
            "Epoch 100/150\n",
            "88/88 - 0s - loss: 1.9313\n",
            "Epoch 101/150\n",
            "88/88 - 0s - loss: 1.9295\n",
            "Epoch 102/150\n",
            "88/88 - 0s - loss: 1.9287\n",
            "Epoch 103/150\n",
            "88/88 - 0s - loss: 1.9288\n",
            "Epoch 104/150\n",
            "88/88 - 0s - loss: 1.9287\n",
            "Epoch 105/150\n",
            "88/88 - 0s - loss: 1.9298\n",
            "Epoch 106/150\n",
            "88/88 - 0s - loss: 1.9266\n",
            "Epoch 107/150\n",
            "88/88 - 0s - loss: 1.9280\n",
            "Epoch 108/150\n",
            "88/88 - 0s - loss: 1.9264\n",
            "Epoch 109/150\n",
            "88/88 - 0s - loss: 1.9278\n",
            "Epoch 110/150\n",
            "88/88 - 0s - loss: 1.9274\n",
            "Epoch 111/150\n",
            "88/88 - 0s - loss: 1.9268\n",
            "Epoch 112/150\n",
            "88/88 - 0s - loss: 1.9273\n",
            "Epoch 113/150\n",
            "88/88 - 0s - loss: 1.9255\n",
            "Epoch 114/150\n",
            "88/88 - 0s - loss: 1.9264\n",
            "Epoch 115/150\n",
            "88/88 - 0s - loss: 1.9260\n",
            "Epoch 116/150\n",
            "88/88 - 0s - loss: 1.9242\n",
            "Epoch 117/150\n",
            "88/88 - 0s - loss: 1.9267\n",
            "Epoch 118/150\n",
            "88/88 - 0s - loss: 1.9247\n",
            "Epoch 119/150\n",
            "88/88 - 0s - loss: 1.9265\n",
            "Epoch 120/150\n",
            "88/88 - 0s - loss: 1.9267\n",
            "Epoch 121/150\n",
            "88/88 - 0s - loss: 1.9255\n",
            "Epoch 122/150\n",
            "88/88 - 0s - loss: 1.9263\n",
            "Epoch 123/150\n",
            "88/88 - 0s - loss: 1.9259\n",
            "Epoch 124/150\n",
            "88/88 - 0s - loss: 1.9238\n",
            "Epoch 125/150\n",
            "88/88 - 0s - loss: 1.9244\n",
            "Epoch 126/150\n",
            "88/88 - 0s - loss: 1.9241\n",
            "Epoch 127/150\n",
            "88/88 - 0s - loss: 1.9237\n",
            "Epoch 128/150\n",
            "88/88 - 0s - loss: 1.9261\n",
            "Epoch 129/150\n",
            "88/88 - 0s - loss: 1.9238\n",
            "Epoch 130/150\n",
            "88/88 - 0s - loss: 1.9230\n",
            "Epoch 131/150\n",
            "88/88 - 0s - loss: 1.9228\n",
            "Epoch 132/150\n",
            "88/88 - 0s - loss: 1.9247\n",
            "Epoch 133/150\n",
            "88/88 - 0s - loss: 1.9240\n",
            "Epoch 134/150\n",
            "88/88 - 0s - loss: 1.9227\n",
            "Epoch 135/150\n",
            "88/88 - 0s - loss: 1.9224\n",
            "Epoch 136/150\n",
            "88/88 - 0s - loss: 1.9225\n",
            "Epoch 137/150\n",
            "88/88 - 0s - loss: 1.9227\n",
            "Epoch 138/150\n",
            "88/88 - 0s - loss: 1.9220\n",
            "Epoch 139/150\n",
            "88/88 - 0s - loss: 1.9223\n",
            "Epoch 140/150\n",
            "88/88 - 0s - loss: 1.9223\n",
            "Epoch 141/150\n",
            "88/88 - 0s - loss: 1.9216\n",
            "Epoch 142/150\n",
            "88/88 - 0s - loss: 1.9228\n",
            "Epoch 143/150\n",
            "88/88 - 0s - loss: 1.9204\n",
            "Epoch 144/150\n",
            "88/88 - 0s - loss: 1.9210\n",
            "Epoch 145/150\n",
            "88/88 - 0s - loss: 1.9209\n",
            "Epoch 146/150\n",
            "88/88 - 0s - loss: 1.9201\n",
            "Epoch 147/150\n",
            "88/88 - 0s - loss: 1.9219\n",
            "Epoch 148/150\n",
            "88/88 - 0s - loss: 1.9219\n",
            "Epoch 149/150\n",
            "88/88 - 0s - loss: 1.9204\n",
            "Epoch 150/150\n",
            "88/88 - 0s - loss: 1.9201\n",
            "Accuracy: 0.280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F4AXAX9mQkS"
      },
      "source": [
        "Accuracy: 0.280"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W95UaQIWmsv7"
      },
      "source": [
        "## Combined Regression and Classification Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59oPJO1WmuHQ",
        "outputId": "9fa87731-91af-43f9-d2fb-b3c10915849c"
      },
      "source": [
        "# mlp for combined regression and classification predictions on the abalone dataset\n",
        "from numpy import unique\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]\n",
        "# encode strings to integer\n",
        "y_class = LabelEncoder().fit_transform(y)\n",
        "n_class = len(unique(y_class))\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test, y_train_class, y_test_class = train_test_split(X, y, y_class, test_size=0.33, random_state=1)\n",
        "# input\n",
        "visible = Input(shape=(n_features,))\n",
        "hidden1 = Dense(20, activation='relu', kernel_initializer='he_normal')(visible)\n",
        "hidden2 = Dense(10, activation='relu', kernel_initializer='he_normal')(hidden1)\n",
        "# regression output\n",
        "out_reg = Dense(1, activation='linear')(hidden2)\n",
        "# classification output\n",
        "out_clas = Dense(n_class, activation='softmax')(hidden2)\n",
        "# define model\n",
        "model = Model(inputs=visible, outputs=[out_reg, out_clas])\n",
        "# compile the keras model\n",
        "model.compile(loss=['mse','sparse_categorical_crossentropy'], optimizer='adam')\n",
        "# plot graph of model\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, [y_train,y_train_class], epochs=150, batch_size=32, verbose=2)\n",
        "# make predictions on test set\n",
        "yhat1, yhat2 = model.predict(X_test)\n",
        "# calculate error for regression model\n",
        "error = mean_absolute_error(y_test, yhat1)\n",
        "print('MAE: %.3f' % error)\n",
        "# evaluate accuracy for classification model\n",
        "yhat2 = argmax(yhat2, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test_class, yhat2)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "88/88 - 0s - loss: 94.8946 - dense_8_loss: 91.6747 - dense_9_loss: 3.2199\n",
            "Epoch 2/150\n",
            "88/88 - 0s - loss: 43.0959 - dense_8_loss: 40.3064 - dense_9_loss: 2.7895\n",
            "Epoch 3/150\n",
            "88/88 - 0s - loss: 14.4825 - dense_8_loss: 11.8916 - dense_9_loss: 2.5909\n",
            "Epoch 4/150\n",
            "88/88 - 0s - loss: 11.3392 - dense_8_loss: 8.8203 - dense_9_loss: 2.5189\n",
            "Epoch 5/150\n",
            "88/88 - 0s - loss: 11.0488 - dense_8_loss: 8.5508 - dense_9_loss: 2.4980\n",
            "Epoch 6/150\n",
            "88/88 - 0s - loss: 10.8091 - dense_8_loss: 8.3215 - dense_9_loss: 2.4876\n",
            "Epoch 7/150\n",
            "88/88 - 0s - loss: 10.5874 - dense_8_loss: 8.1084 - dense_9_loss: 2.4790\n",
            "Epoch 8/150\n",
            "88/88 - 0s - loss: 10.3884 - dense_8_loss: 7.9161 - dense_9_loss: 2.4723\n",
            "Epoch 9/150\n",
            "88/88 - 0s - loss: 10.2151 - dense_8_loss: 7.7506 - dense_9_loss: 2.4646\n",
            "Epoch 10/150\n",
            "88/88 - 0s - loss: 10.0690 - dense_8_loss: 7.6111 - dense_9_loss: 2.4579\n",
            "Epoch 11/150\n",
            "88/88 - 0s - loss: 9.9378 - dense_8_loss: 7.4883 - dense_9_loss: 2.4496\n",
            "Epoch 12/150\n",
            "88/88 - 0s - loss: 9.8377 - dense_8_loss: 7.3966 - dense_9_loss: 2.4410\n",
            "Epoch 13/150\n",
            "88/88 - 0s - loss: 9.7322 - dense_8_loss: 7.2999 - dense_9_loss: 2.4322\n",
            "Epoch 14/150\n",
            "88/88 - 0s - loss: 9.6539 - dense_8_loss: 7.2304 - dense_9_loss: 2.4235\n",
            "Epoch 15/150\n",
            "88/88 - 0s - loss: 9.5731 - dense_8_loss: 7.1587 - dense_9_loss: 2.4144\n",
            "Epoch 16/150\n",
            "88/88 - 0s - loss: 9.5031 - dense_8_loss: 7.0976 - dense_9_loss: 2.4056\n",
            "Epoch 17/150\n",
            "88/88 - 0s - loss: 9.4190 - dense_8_loss: 7.0261 - dense_9_loss: 2.3928\n",
            "Epoch 18/150\n",
            "88/88 - 0s - loss: 9.3446 - dense_8_loss: 6.9618 - dense_9_loss: 2.3828\n",
            "Epoch 19/150\n",
            "88/88 - 0s - loss: 9.2492 - dense_8_loss: 6.8742 - dense_9_loss: 2.3750\n",
            "Epoch 20/150\n",
            "88/88 - 0s - loss: 9.1390 - dense_8_loss: 6.7724 - dense_9_loss: 2.3666\n",
            "Epoch 21/150\n",
            "88/88 - 0s - loss: 9.0266 - dense_8_loss: 6.6681 - dense_9_loss: 2.3585\n",
            "Epoch 22/150\n",
            "88/88 - 0s - loss: 8.8977 - dense_8_loss: 6.5457 - dense_9_loss: 2.3520\n",
            "Epoch 23/150\n",
            "88/88 - 0s - loss: 8.7820 - dense_8_loss: 6.4369 - dense_9_loss: 2.3451\n",
            "Epoch 24/150\n",
            "88/88 - 0s - loss: 8.6688 - dense_8_loss: 6.3285 - dense_9_loss: 2.3403\n",
            "Epoch 25/150\n",
            "88/88 - 0s - loss: 8.5179 - dense_8_loss: 6.1822 - dense_9_loss: 2.3358\n",
            "Epoch 26/150\n",
            "88/88 - 0s - loss: 8.4410 - dense_8_loss: 6.1078 - dense_9_loss: 2.3331\n",
            "Epoch 27/150\n",
            "88/88 - 0s - loss: 8.3313 - dense_8_loss: 6.0031 - dense_9_loss: 2.3282\n",
            "Epoch 28/150\n",
            "88/88 - 0s - loss: 8.2317 - dense_8_loss: 5.9069 - dense_9_loss: 2.3248\n",
            "Epoch 29/150\n",
            "88/88 - 0s - loss: 8.1239 - dense_8_loss: 5.7997 - dense_9_loss: 2.3242\n",
            "Epoch 30/150\n",
            "88/88 - 0s - loss: 8.0719 - dense_8_loss: 5.7513 - dense_9_loss: 2.3205\n",
            "Epoch 31/150\n",
            "88/88 - 0s - loss: 7.9414 - dense_8_loss: 5.6223 - dense_9_loss: 2.3191\n",
            "Epoch 32/150\n",
            "88/88 - 0s - loss: 7.8560 - dense_8_loss: 5.5414 - dense_9_loss: 2.3146\n",
            "Epoch 33/150\n",
            "88/88 - 0s - loss: 7.7704 - dense_8_loss: 5.4572 - dense_9_loss: 2.3133\n",
            "Epoch 34/150\n",
            "88/88 - 0s - loss: 7.7328 - dense_8_loss: 5.4232 - dense_9_loss: 2.3096\n",
            "Epoch 35/150\n",
            "88/88 - 0s - loss: 7.6631 - dense_8_loss: 5.3569 - dense_9_loss: 2.3062\n",
            "Epoch 36/150\n",
            "88/88 - 0s - loss: 7.6142 - dense_8_loss: 5.3097 - dense_9_loss: 2.3044\n",
            "Epoch 37/150\n",
            "88/88 - 0s - loss: 7.5634 - dense_8_loss: 5.2597 - dense_9_loss: 2.3037\n",
            "Epoch 38/150\n",
            "88/88 - 0s - loss: 7.5345 - dense_8_loss: 5.2340 - dense_9_loss: 2.3005\n",
            "Epoch 39/150\n",
            "88/88 - 0s - loss: 7.4696 - dense_8_loss: 5.1721 - dense_9_loss: 2.2975\n",
            "Epoch 40/150\n",
            "88/88 - 0s - loss: 7.4574 - dense_8_loss: 5.1594 - dense_9_loss: 2.2980\n",
            "Epoch 41/150\n",
            "88/88 - 0s - loss: 7.4379 - dense_8_loss: 5.1421 - dense_9_loss: 2.2959\n",
            "Epoch 42/150\n",
            "88/88 - 0s - loss: 7.4427 - dense_8_loss: 5.1490 - dense_9_loss: 2.2937\n",
            "Epoch 43/150\n",
            "88/88 - 0s - loss: 7.4079 - dense_8_loss: 5.1137 - dense_9_loss: 2.2941\n",
            "Epoch 44/150\n",
            "88/88 - 0s - loss: 7.3529 - dense_8_loss: 5.0621 - dense_9_loss: 2.2908\n",
            "Epoch 45/150\n",
            "88/88 - 0s - loss: 7.3634 - dense_8_loss: 5.0751 - dense_9_loss: 2.2883\n",
            "Epoch 46/150\n",
            "88/88 - 0s - loss: 7.3584 - dense_8_loss: 5.0708 - dense_9_loss: 2.2876\n",
            "Epoch 47/150\n",
            "88/88 - 0s - loss: 7.3614 - dense_8_loss: 5.0759 - dense_9_loss: 2.2855\n",
            "Epoch 48/150\n",
            "88/88 - 0s - loss: 7.3024 - dense_8_loss: 5.0153 - dense_9_loss: 2.2871\n",
            "Epoch 49/150\n",
            "88/88 - 0s - loss: 7.3020 - dense_8_loss: 5.0194 - dense_9_loss: 2.2826\n",
            "Epoch 50/150\n",
            "88/88 - 0s - loss: 7.3050 - dense_8_loss: 5.0249 - dense_9_loss: 2.2801\n",
            "Epoch 51/150\n",
            "88/88 - 0s - loss: 7.2836 - dense_8_loss: 5.0055 - dense_9_loss: 2.2780\n",
            "Epoch 52/150\n",
            "88/88 - 0s - loss: 7.3116 - dense_8_loss: 5.0353 - dense_9_loss: 2.2763\n",
            "Epoch 53/150\n",
            "88/88 - 0s - loss: 7.2470 - dense_8_loss: 4.9727 - dense_9_loss: 2.2743\n",
            "Epoch 54/150\n",
            "88/88 - 0s - loss: 7.2972 - dense_8_loss: 5.0249 - dense_9_loss: 2.2723\n",
            "Epoch 55/150\n",
            "88/88 - 0s - loss: 7.2961 - dense_8_loss: 5.0266 - dense_9_loss: 2.2695\n",
            "Epoch 56/150\n",
            "88/88 - 0s - loss: 7.2665 - dense_8_loss: 4.9989 - dense_9_loss: 2.2677\n",
            "Epoch 57/150\n",
            "88/88 - 0s - loss: 7.2415 - dense_8_loss: 4.9758 - dense_9_loss: 2.2657\n",
            "Epoch 58/150\n",
            "88/88 - 0s - loss: 7.2356 - dense_8_loss: 4.9716 - dense_9_loss: 2.2640\n",
            "Epoch 59/150\n",
            "88/88 - 0s - loss: 7.2512 - dense_8_loss: 4.9899 - dense_9_loss: 2.2613\n",
            "Epoch 60/150\n",
            "88/88 - 0s - loss: 7.2436 - dense_8_loss: 4.9837 - dense_9_loss: 2.2599\n",
            "Epoch 61/150\n",
            "88/88 - 0s - loss: 7.2567 - dense_8_loss: 4.9978 - dense_9_loss: 2.2589\n",
            "Epoch 62/150\n",
            "88/88 - 0s - loss: 7.2353 - dense_8_loss: 4.9799 - dense_9_loss: 2.2555\n",
            "Epoch 63/150\n",
            "88/88 - 0s - loss: 7.2039 - dense_8_loss: 4.9496 - dense_9_loss: 2.2543\n",
            "Epoch 64/150\n",
            "88/88 - 0s - loss: 7.2150 - dense_8_loss: 4.9631 - dense_9_loss: 2.2519\n",
            "Epoch 65/150\n",
            "88/88 - 0s - loss: 7.2120 - dense_8_loss: 4.9630 - dense_9_loss: 2.2490\n",
            "Epoch 66/150\n",
            "88/88 - 0s - loss: 7.1986 - dense_8_loss: 4.9505 - dense_9_loss: 2.2481\n",
            "Epoch 67/150\n",
            "88/88 - 0s - loss: 7.2077 - dense_8_loss: 4.9629 - dense_9_loss: 2.2448\n",
            "Epoch 68/150\n",
            "88/88 - 0s - loss: 7.2006 - dense_8_loss: 4.9568 - dense_9_loss: 2.2438\n",
            "Epoch 69/150\n",
            "88/88 - 0s - loss: 7.1822 - dense_8_loss: 4.9402 - dense_9_loss: 2.2420\n",
            "Epoch 70/150\n",
            "88/88 - 0s - loss: 7.1977 - dense_8_loss: 4.9564 - dense_9_loss: 2.2414\n",
            "Epoch 71/150\n",
            "88/88 - 0s - loss: 7.1739 - dense_8_loss: 4.9357 - dense_9_loss: 2.2382\n",
            "Epoch 72/150\n",
            "88/88 - 0s - loss: 7.1565 - dense_8_loss: 4.9194 - dense_9_loss: 2.2372\n",
            "Epoch 73/150\n",
            "88/88 - 0s - loss: 7.1556 - dense_8_loss: 4.9208 - dense_9_loss: 2.2347\n",
            "Epoch 74/150\n",
            "88/88 - 0s - loss: 7.1848 - dense_8_loss: 4.9497 - dense_9_loss: 2.2351\n",
            "Epoch 75/150\n",
            "88/88 - 0s - loss: 7.1851 - dense_8_loss: 4.9545 - dense_9_loss: 2.2306\n",
            "Epoch 76/150\n",
            "88/88 - 0s - loss: 7.1543 - dense_8_loss: 4.9253 - dense_9_loss: 2.2290\n",
            "Epoch 77/150\n",
            "88/88 - 0s - loss: 7.1475 - dense_8_loss: 4.9202 - dense_9_loss: 2.2273\n",
            "Epoch 78/150\n",
            "88/88 - 0s - loss: 7.1344 - dense_8_loss: 4.9077 - dense_9_loss: 2.2267\n",
            "Epoch 79/150\n",
            "88/88 - 0s - loss: 7.1539 - dense_8_loss: 4.9299 - dense_9_loss: 2.2239\n",
            "Epoch 80/150\n",
            "88/88 - 0s - loss: 7.1321 - dense_8_loss: 4.9108 - dense_9_loss: 2.2213\n",
            "Epoch 81/150\n",
            "88/88 - 0s - loss: 7.1317 - dense_8_loss: 4.9120 - dense_9_loss: 2.2196\n",
            "Epoch 82/150\n",
            "88/88 - 0s - loss: 7.1252 - dense_8_loss: 4.9075 - dense_9_loss: 2.2178\n",
            "Epoch 83/150\n",
            "88/88 - 0s - loss: 7.1320 - dense_8_loss: 4.9161 - dense_9_loss: 2.2160\n",
            "Epoch 84/150\n",
            "88/88 - 0s - loss: 7.1181 - dense_8_loss: 4.9037 - dense_9_loss: 2.2144\n",
            "Epoch 85/150\n",
            "88/88 - 0s - loss: 7.1216 - dense_8_loss: 4.9092 - dense_9_loss: 2.2124\n",
            "Epoch 86/150\n",
            "88/88 - 0s - loss: 7.1424 - dense_8_loss: 4.9308 - dense_9_loss: 2.2116\n",
            "Epoch 87/150\n",
            "88/88 - 0s - loss: 7.1112 - dense_8_loss: 4.9021 - dense_9_loss: 2.2091\n",
            "Epoch 88/150\n",
            "88/88 - 0s - loss: 7.0882 - dense_8_loss: 4.8794 - dense_9_loss: 2.2088\n",
            "Epoch 89/150\n",
            "88/88 - 0s - loss: 7.1146 - dense_8_loss: 4.9092 - dense_9_loss: 2.2054\n",
            "Epoch 90/150\n",
            "88/88 - 0s - loss: 7.0951 - dense_8_loss: 4.8919 - dense_9_loss: 2.2033\n",
            "Epoch 91/150\n",
            "88/88 - 0s - loss: 7.1042 - dense_8_loss: 4.9011 - dense_9_loss: 2.2031\n",
            "Epoch 92/150\n",
            "88/88 - 0s - loss: 7.0536 - dense_8_loss: 4.8522 - dense_9_loss: 2.2014\n",
            "Epoch 93/150\n",
            "88/88 - 0s - loss: 7.0957 - dense_8_loss: 4.8978 - dense_9_loss: 2.1979\n",
            "Epoch 94/150\n",
            "88/88 - 0s - loss: 7.1223 - dense_8_loss: 4.9239 - dense_9_loss: 2.1984\n",
            "Epoch 95/150\n",
            "88/88 - 0s - loss: 7.0749 - dense_8_loss: 4.8800 - dense_9_loss: 2.1949\n",
            "Epoch 96/150\n",
            "88/88 - 0s - loss: 7.0758 - dense_8_loss: 4.8828 - dense_9_loss: 2.1930\n",
            "Epoch 97/150\n",
            "88/88 - 0s - loss: 7.0752 - dense_8_loss: 4.8833 - dense_9_loss: 2.1919\n",
            "Epoch 98/150\n",
            "88/88 - 0s - loss: 7.0875 - dense_8_loss: 4.8976 - dense_9_loss: 2.1899\n",
            "Epoch 99/150\n",
            "88/88 - 0s - loss: 7.1112 - dense_8_loss: 4.9224 - dense_9_loss: 2.1888\n",
            "Epoch 100/150\n",
            "88/88 - 0s - loss: 7.0634 - dense_8_loss: 4.8768 - dense_9_loss: 2.1867\n",
            "Epoch 101/150\n",
            "88/88 - 0s - loss: 7.0563 - dense_8_loss: 4.8702 - dense_9_loss: 2.1861\n",
            "Epoch 102/150\n",
            "88/88 - 0s - loss: 7.0646 - dense_8_loss: 4.8812 - dense_9_loss: 2.1834\n",
            "Epoch 103/150\n",
            "88/88 - 0s - loss: 7.0945 - dense_8_loss: 4.9140 - dense_9_loss: 2.1805\n",
            "Epoch 104/150\n",
            "88/88 - 0s - loss: 7.0428 - dense_8_loss: 4.8624 - dense_9_loss: 2.1804\n",
            "Epoch 105/150\n",
            "88/88 - 0s - loss: 7.0398 - dense_8_loss: 4.8617 - dense_9_loss: 2.1781\n",
            "Epoch 106/150\n",
            "88/88 - 0s - loss: 7.0442 - dense_8_loss: 4.8675 - dense_9_loss: 2.1768\n",
            "Epoch 107/150\n",
            "88/88 - 0s - loss: 7.0452 - dense_8_loss: 4.8696 - dense_9_loss: 2.1756\n",
            "Epoch 108/150\n",
            "88/88 - 0s - loss: 7.0437 - dense_8_loss: 4.8697 - dense_9_loss: 2.1740\n",
            "Epoch 109/150\n",
            "88/88 - 0s - loss: 7.0287 - dense_8_loss: 4.8567 - dense_9_loss: 2.1720\n",
            "Epoch 110/150\n",
            "88/88 - 0s - loss: 7.0351 - dense_8_loss: 4.8650 - dense_9_loss: 2.1702\n",
            "Epoch 111/150\n",
            "88/88 - 0s - loss: 7.0622 - dense_8_loss: 4.8929 - dense_9_loss: 2.1693\n",
            "Epoch 112/150\n",
            "88/88 - 0s - loss: 7.0313 - dense_8_loss: 4.8634 - dense_9_loss: 2.1679\n",
            "Epoch 113/150\n",
            "88/88 - 0s - loss: 7.0169 - dense_8_loss: 4.8506 - dense_9_loss: 2.1663\n",
            "Epoch 114/150\n",
            "88/88 - 0s - loss: 7.0526 - dense_8_loss: 4.8881 - dense_9_loss: 2.1644\n",
            "Epoch 115/150\n",
            "88/88 - 0s - loss: 7.0158 - dense_8_loss: 4.8537 - dense_9_loss: 2.1621\n",
            "Epoch 116/150\n",
            "88/88 - 0s - loss: 7.0175 - dense_8_loss: 4.8562 - dense_9_loss: 2.1613\n",
            "Epoch 117/150\n",
            "88/88 - 0s - loss: 6.9897 - dense_8_loss: 4.8305 - dense_9_loss: 2.1592\n",
            "Epoch 118/150\n",
            "88/88 - 0s - loss: 6.9942 - dense_8_loss: 4.8354 - dense_9_loss: 2.1588\n",
            "Epoch 119/150\n",
            "88/88 - 0s - loss: 6.9990 - dense_8_loss: 4.8425 - dense_9_loss: 2.1566\n",
            "Epoch 120/150\n",
            "88/88 - 0s - loss: 7.0152 - dense_8_loss: 4.8597 - dense_9_loss: 2.1554\n",
            "Epoch 121/150\n",
            "88/88 - 0s - loss: 6.9951 - dense_8_loss: 4.8418 - dense_9_loss: 2.1533\n",
            "Epoch 122/150\n",
            "88/88 - 0s - loss: 7.0020 - dense_8_loss: 4.8499 - dense_9_loss: 2.1521\n",
            "Epoch 123/150\n",
            "88/88 - 0s - loss: 7.0115 - dense_8_loss: 4.8616 - dense_9_loss: 2.1499\n",
            "Epoch 124/150\n",
            "88/88 - 0s - loss: 6.9936 - dense_8_loss: 4.8447 - dense_9_loss: 2.1489\n",
            "Epoch 125/150\n",
            "88/88 - 0s - loss: 6.9771 - dense_8_loss: 4.8287 - dense_9_loss: 2.1484\n",
            "Epoch 126/150\n",
            "88/88 - 0s - loss: 6.9728 - dense_8_loss: 4.8264 - dense_9_loss: 2.1463\n",
            "Epoch 127/150\n",
            "88/88 - 0s - loss: 7.0132 - dense_8_loss: 4.8685 - dense_9_loss: 2.1447\n",
            "Epoch 128/150\n",
            "88/88 - 0s - loss: 7.0682 - dense_8_loss: 4.9237 - dense_9_loss: 2.1444\n",
            "Epoch 129/150\n",
            "88/88 - 0s - loss: 6.9594 - dense_8_loss: 4.8178 - dense_9_loss: 2.1416\n",
            "Epoch 130/150\n",
            "88/88 - 0s - loss: 6.9519 - dense_8_loss: 4.8121 - dense_9_loss: 2.1398\n",
            "Epoch 131/150\n",
            "88/88 - 0s - loss: 6.9782 - dense_8_loss: 4.8381 - dense_9_loss: 2.1401\n",
            "Epoch 132/150\n",
            "88/88 - 0s - loss: 6.9617 - dense_8_loss: 4.8243 - dense_9_loss: 2.1374\n",
            "Epoch 133/150\n",
            "88/88 - 0s - loss: 6.9658 - dense_8_loss: 4.8298 - dense_9_loss: 2.1360\n",
            "Epoch 134/150\n",
            "88/88 - 0s - loss: 6.9616 - dense_8_loss: 4.8256 - dense_9_loss: 2.1360\n",
            "Epoch 135/150\n",
            "88/88 - 0s - loss: 6.9531 - dense_8_loss: 4.8184 - dense_9_loss: 2.1347\n",
            "Epoch 136/150\n",
            "88/88 - 0s - loss: 6.9533 - dense_8_loss: 4.8219 - dense_9_loss: 2.1314\n",
            "Epoch 137/150\n",
            "88/88 - 0s - loss: 6.9596 - dense_8_loss: 4.8290 - dense_9_loss: 2.1306\n",
            "Epoch 138/150\n",
            "88/88 - 0s - loss: 6.9436 - dense_8_loss: 4.8141 - dense_9_loss: 2.1295\n",
            "Epoch 139/150\n",
            "88/88 - 0s - loss: 6.9426 - dense_8_loss: 4.8139 - dense_9_loss: 2.1287\n",
            "Epoch 140/150\n",
            "88/88 - 0s - loss: 6.9141 - dense_8_loss: 4.7883 - dense_9_loss: 2.1257\n",
            "Epoch 141/150\n",
            "88/88 - 0s - loss: 6.9346 - dense_8_loss: 4.8091 - dense_9_loss: 2.1255\n",
            "Epoch 142/150\n",
            "88/88 - 0s - loss: 6.9713 - dense_8_loss: 4.8479 - dense_9_loss: 2.1234\n",
            "Epoch 143/150\n",
            "88/88 - 0s - loss: 6.9408 - dense_8_loss: 4.8200 - dense_9_loss: 2.1208\n",
            "Epoch 144/150\n",
            "88/88 - 0s - loss: 6.9260 - dense_8_loss: 4.8040 - dense_9_loss: 2.1220\n",
            "Epoch 145/150\n",
            "88/88 - 0s - loss: 6.9261 - dense_8_loss: 4.8063 - dense_9_loss: 2.1197\n",
            "Epoch 146/150\n",
            "88/88 - 0s - loss: 6.9269 - dense_8_loss: 4.8076 - dense_9_loss: 2.1193\n",
            "Epoch 147/150\n",
            "88/88 - 0s - loss: 6.9228 - dense_8_loss: 4.8070 - dense_9_loss: 2.1158\n",
            "Epoch 148/150\n",
            "88/88 - 0s - loss: 6.9172 - dense_8_loss: 4.8024 - dense_9_loss: 2.1148\n",
            "Epoch 149/150\n",
            "88/88 - 0s - loss: 6.9041 - dense_8_loss: 4.7900 - dense_9_loss: 2.1142\n",
            "Epoch 150/150\n",
            "88/88 - 0s - loss: 6.9280 - dense_8_loss: 4.8155 - dense_9_loss: 2.1125\n",
            "MAE: 1.576\n",
            "Accuracy: 0.250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxaQgrDYhPUg"
      },
      "source": [
        "## Comentarios\n",
        "Hasta el momento todo claro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95W1DCY5hQ5_"
      },
      "source": [
        "## Reporte\n",
        "### Resumen\n",
        "Cuando se desea predecir dos valores, uno num√©rico y uno categ√≥rico, se pueden presentar problemas. A continuaci√≥n se van a presentar dos posibles formas de solucionar este problema:\n",
        "* Utilizar un modelo por cada valor necesario: La desventaja de este m√©todo es que las predicciones de los modelos pueden no calzar.\n",
        "* Un √∫nico modelo capaz de realizar distintas predicciones: Es conocido como un modelo multi-salida. Este tipo de modelo tiene como beneficio que solo se requiere implementar y mantener uno √∫nico, en vez de estar realizando estas tareas en m√∫ltiples modelos por cada valor que se desee predecir. Esto √∫ltimo tambi√©n puede ofrecer mayor consistencia en las predicciones.\n",
        " \n",
        "### Comentarios\n",
        "No hab√≠a considerado que pod√≠an existir modelos que realizar√°n m√°s de una predicci√≥n como salida.\n",
        " \n",
        "### Dudas\n",
        "Por el momento no tengo dudas."
      ]
    }
  ]
}